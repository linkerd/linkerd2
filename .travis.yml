---
dist: trusty
sudo: false

# We do not test pushes to branches, since they are redundant with the pull_request build
# for each branch. Take that, Big CI!
branches:
  only:
    - master

stages:
  - name: test
  - name: docker-deploy
    if: branch = master AND type != pull_request

jobs:
  include:

    # Compile the application and run tests.
    - stage: test
      language: rust
      rust: stable
      cache: cargo
      script:
        - (cd proxy && cargo test --locked --no-default-features)

    - language: rust
      # Run proxy benchmarks in development mode. Requires nightly.
      # Failures are not fatal.
      rust: nightly
      cache: cargo
      script:
        - (cd proxy && cargo test --benches --locked --no-default-features)

    - language: go
      # Quote the version number to avoid parsing issues like
      # https://github.com/travis-ci/gimme/issues/132.
      go: "1.10.2"
      go_import_path: github.com/runconduit/conduit
      cache:
        directories:
          - vendor
      install:
        - ./bin/dep ensure -vendor-only -v
        - ./bin/dep status -v
      script:
        # TODO decide whether protoc should be committed or not. If so, we shouldn't do
        # this or we should error if it dirties the repo.
        - ./bin/protoc-go.sh
        - go test -race -v ./...
        - go vet ./...

    - language: node_js
      node_js:
        - "8"
      cache:
        directories:
          - web/app/node_modules
      before_install:
        - curl -o- -L https://yarnpkg.com/install.sh | bash -s -- --version 1.3.2
        - export PATH="$HOME/.yarn/bin:$PATH"
      install:
        - ./bin/web
      script:
        - ./bin/web test --reporters dots

    - language: generic
      script:
        - |
          (
            . bin/_tag.sh
            for f in $( grep -lR --include=Dockerfile\* go-deps: . ) ; do
              validate_go_deps_tag $f
            done
          )

    # Push container images to Google Container Registry.
    - stage: docker-deploy

      language: generic
      services:
        - docker

      cache:
        directories:
          - "$HOME/google-cloud-sdk/"
          - "$HOME/.cache"

      before_install:
        - docker version

        - |
          # Install gcloud and kubectl.
          dir="${CLOUDSDK_INSTALL_DIR:-${HOME}}/google-cloud-sdk"
          export CLOUDSDK_CORE_DISABLE_PROMPTS=1
          if [ -d "$dir/bin" ]; then
              . "$dir/path.bash.inc"
              gcloud components update
          else
              rm -rf "$dir"
              curl https://sdk.cloud.google.com | bash
              . "$dir/path.bash.inc"
          fi
          gcloud components install kubectl
        - |
          # Configure gcloud with a service account.
          openssl aes-256-cbc -K $encrypted_6af64675f81c_key -iv $encrypted_6af64675f81c_iv -in .gcp.json.enc -out .gcp.json -d
          gcloud auth activate-service-account --key-file .gcp.json
          gcloud config set core/project "$GCP_PROJECT"
          gcloud config set compute/zone "$GCP_ZONE"
          gcloud config set container/cluster "$GKE_CLUSTER"
        - |
          # Get a kubernetes context.
          (. bin/_gcp.sh ; get_k8s_ctx "$GCP_PROJECT" "$GCP_ZONE" "$GKE_CLUSTER")
        - gcloud version
        - kubectl version --short

      before_script:
        - gcloud docker --authorize-only
        - bin/docker-pull-deps
         # Pulling master helps with docker build cache, but may fail if we're
         # adding a new image to the mix.
        - bin/docker-pull master || echo "docker pull failed" >&2
        - |
          export CONDUIT_TAG=$(. bin/_tag.sh ; clean_head_root_tag)
          echo "CONDUIT_TAG=${CONDUIT_TAG}"
        - export BUILD_DEBUG=1 DOCKER_TRACE=1
        - export DOCKER_CACHE="$HOME/.cache/docker.tgz" PROXY_BUILD_CACHE_IMAGE="proxy-build:cache"
        - if [ -f "$DOCKER_CACHE" ]; then gunzip -c "$DOCKER_CACHE" | docker load ; docker image ls "$PROXY_BUILD_CACHE_IMAGE" ; fi

      script:
        # Build the proxy so that its build stage can be cached for reuse.
        - docker build . -f ./proxy/Dockerfile --build-arg="PROXY_UNOPTIMIZED=${PROXY_UNOPTIMIZED:-}" --cache-from="$PROXY_BUILD_CACHE_IMAGE" --tag "$PROXY_BUILD_CACHE_IMAGE" --target build
        # bin/docker-build-proxy uses PROXY_BUILD_CACHE_IMAGE.
        - bin/docker-build
        # The proxy's build cache is preserved so it can be reused on subsequent runs.
        - docker save gcr.io/runconduit/proxy-build:cache | gzip -9 >"$DOCKER_CACHE"

      after_success:
        - bin/docker-push-deps
        - bin/docker-push $CONDUIT_TAG
        - bin/docker-retag-all $CONDUIT_TAG master && bin/docker-push master
        - target/cli/linux/conduit install --conduit-version=$CONDUIT_TAG |tee conduit.yml
        - kubectl -n conduit apply -f conduit.yml --prune --selector='conduit.io/control-plane-component'

  allow_failures:
    - rust: nightly
  fast_finish: true

notifications:
  email:
    on_success: never
